```julia

using Revise

using Weave
Weave.set_chunk_defaults(Dict{Symbol,Any}(
    :results => "hidden",
    :echo => false
))

using 
    LinearAlgebra, 
    Random,
    Statistics

using 
    Plots,
    PlotThemes,
    StatsPlots,
    RecipesBase,
    Colors, 
    Images,
    DataFrames,
    DataFramesMeta,
    Underscore,
    StatsBase,
    ConjugatePriors,
    Particles,
    Distances, 
    Bootstrap

theme(:default, markerstrokecolor=:white)

flip(x::AbstractVector) = reshape(x, (1,:))

const It = Base.Iterators

includet("../modeling.jl")
using .DotLearning
includet("../experiments.jl")
using .Experiments
includet("../plots.jl")

using JLD2
@load "../data/dots2014.jld2" recall pred
# recall data from one subject
one_subj = 4
recall1 = @where(recall, :subjid1 .== one_subj)

# load the prior _parameters_ to get around
@load "../prior_empirical_params.jld2"
prior_optimized = ConjugatePriors.NormalInverseWishart(μ, κ, Λ, ν)
import .Experiments: cosinesim
cosinesim(d) = cosinesim(d,d)

rho(args...) = sqrt(sum(args.^2))

boot_ci(x, f=mean, n=1000, cil=0.95) = confint(bootstrap(f, x, BasicSampling(n)),
                                               BasicConfInt(cil))

nblock = length(unique(recall1[:block]))

block_pallete = LCHab.(60, 60, range(0, 320, length=nblock))

```


# Task

```{julia; label="task-flow-1"}
x, y = -0.3, 0.4
arena([x], [y], color="black")
```

```{julia; label="task-flow-2"}
plot(Gray.(rand(Bool, 100,100)), axis=false, lims=(0,100), aspect_ratio=:equal)
```

```{julia; label="task-flow-3"}
arena([x], [y], color=:white, markerstrokecolor=:black, markersize=5)
annotate!(0,0, text("?", 32))
```


```{julia; label="studied-locations"}

p1 = plot(range(0, 2π, length=100), [2π], proj=:polar, color=:black,
          axis=false, grid=false, legend=false,
          title="Studied locations")
@df recall1 scatter!(p1, :theta, :rho, color=:black, markeralpha=0.5)

p2 = plot(range(0, 2π, length=100), [2π], proj=:polar, color=:black,
          palette = block_pallete, axis=false, grid=false,
          title="Colored by block")
@_ recall1 |>
    by(_, [:block], :theta=>mean, :rho=>mean) |>
    @df(_, plot!(p2, :theta_mean, :rho_mean, color=GrayA(0.5, 0.2)))
@_ recall1 |>
    by(_, [:block], :theta=>mean, :rho=>mean) |>
    @df(_, scatter!(p2, :theta_mean, :rho_mean, group=:block,
                    c=(1:nblock)',
                    markersize=8, markeralpha=0.2))
@df recall1 scatter!(p2, :theta, :rho, group=:block, proj=^(:polar),
                        palette = block_pallete, legend=false)

plot(p1, p2, size=(850, 400))

```

# Behavioral data

```{julia; label="behavioral-data"}
@df recall1 arena(:x, :y, quiver=(:x_resp .- :x, :y_resp .- :y),
                    seriestype=:quiver, color=GrayA(0.0, 1.0), label="Behavior")
```

```{julia; label="known-clusters"}

known_recalled = by(recall, :subjid1) do d
    @_ KnownFilter(prior_optimized) |>
    RecallFilter(_, Matrix(0.01I,2,2)) |>
    filter!(_, extract_data(d, _)) |>
    DataFrame |>
    hcat(d, _) |>
    deletecols!(_, :subjid1) |>
    @transform(_, cosinesim = 1 .- Distances.colwise(CosineDist(), 
                                                   hcat(:x_mod.-:x, :y_mod.-:y)', 
                                                   hcat(:x_resp.-:x, :y_resp.-:y)'))
end

known_recalled1 = @where(known_recalled, :subjid1 .== one_subj)

arena()
@_ known_recalled1 |>
    by(_, :block, :x=>mean, :y=>mean) |>
    @df(_, scatter!(:x_mean, :y_mean, color=block_pallete,
                    markersize=8, markeralpha=0.4))
@df recall1 quiver!(:x, :y, quiver=(:x_resp .- :x, :y_resp .- :y),
                    seriestype=:quiver, color=GrayA(0.0, 0.5), label="Behavior")
@df known_recalled1 plot!(:x, :y, quiver=(:x_mod .- :x, :y_mod .- :y),
                          seriestype=:quiver, group=:block, color=(1:25)',
                          palette = RGBA.(block_pallete, 0.7),
                          label="Cluster bias", markeralpha=0.5)

```

# Results

## Recovering clusters

### Example

```{julia; }
Random.seed!(2019)

rf_cl = RecallFilter(ChenLiuParticles(100,
                                      prior_optimized,
                                      StickyCRP(0.01, 0.9)),
                     Matrix(0.01I,2,2))
rf_fh = RecallFilter(FearnheadParticles(100,
                                        prior_optimized,
                                        StickyCRP(0.01, 0.9)),
                     Matrix(0.01I,2,2))

filter!(rf_cl, extract_data(recall1, rf_cl))
filter!(rf_fh, extract_data(recall1, rf_fh))

rf = rf_fh

filter_sim = assignment_similarity(rf)
filter_sim_cl = assignment_similarity(rf_cl)

# outline the boundaries of the blocks in ~colors~
function outline_blocks(sim_mat, blocks, block_pallete)
    block_colors = RGB.(Gray.(sim_mat))
    nblock = length(unique(blocks))
    for n in 1:nblock
        start = findfirst(isequal(n), blocks)
        stop = findlast(isequal(n), blocks)
        color = block_pallete[n]
        block = view(block_colors, start:stop, start:stop)
        block[1:end, 1] .= color
        block[1:end, end] .= color
        block[1, 1:end] .= color
        block[end, 1:end] .= color
    end
    return block_colors
end

euc_dist = @_ recall1 |>
    @with(_, hcat(:x, :y)) |>
    Distances.pairwise(Euclidean(), _', dims=2)



save("figures/inferred-blocks.png", outline_blocks(filter_sim, recall1[:block], block_pallete))

# plot(plot(outline_blocks(filter_sim, recall1[:block], block_pallete),
#           title="Inferred clusters (FC)", guide="Trial"),
#      plot(outline_blocks(filter_sim_cl, recall1[:block], block_pallete),
#           title="Inferred clusters (CL)", guide="Trial"),
#      plot(outline_blocks(1 .- euc_dist ./ maximum(euc_dist),
#                          recall1[:block],
#                          block_pallete),
#           title="Euclidean distance"),
#      aspect_ratio=:equal, layout=(1,3), size=(900,300))

plot(plot(outline_blocks(filter_sim, recall1[:block], block_pallete),
          title="Inferred clusters", guide="Trial"),
     plot(outline_blocks(1 .- euc_dist ./ maximum(euc_dist),
                         recall1[:block],
                         block_pallete),
          title="Euclidean distance"),
     aspect_ratio=:equal, layout=(1,2), size=(600,300))


```

```{julia; label="inferred-clusters"}
plot(outline_blocks(filter_sim, recall1[:block], block_pallete),
     title="Inferred clusters", guide="Trial",
     aspect_ratio=:equal, size=(300,300), grid = false, ticks=true)
```

### Adjusted Rand Index

```{julia; label="adjusted-rand-index"}

@load "../results/randindex-2019-07-15T16:17:46.864.jld2" results_df

ari_summaries = by(results_df, [:α, :ρ], :ari => x -> NamedTuple{(:mean, :low, :high)}(boot_ci(x)...))

ari_bysub = by(results_df, [:α, :ρ, :subjid1], ari = :ari => mean)
ari_summaries_bysub = by(ari_bysub, [:α, :ρ], :ari => x -> NamedTuple{(:mean, :low, :high)}(boot_ci(x)...))

@df ari_summaries_bysub plot(:α, :mean, ribbon=(:mean .- :low, :high .- :mean),
                             group=:ρ, xscale=:log10, ylim=(0.75, 1.0),
                             xlabel="Eagerness to create new clusters (concentration \\alpha)",
                             ylabel=("Adjusted Rand Index with true clusters"),
                             legend=:bottomright, legend_title="Stickiness \\rho")

```

## Recall

```julia

@load "../results/run3-2019-01-07T21:23:16.243-recalled-predicted.jld2" recalled_all predicted_all

summaries_by_iter = by(recalled_all, [:α, :ρ, :Sσ, :iter]) do d
    DataFrame(cos=cosinesim(d), mse=Experiments.mse(d,d))
end

recalled_summaries = by(summaries_by_iter, [:α, :ρ, :Sσ]) do d
    # collect only necessary here because of Bootstrap.jl#42, can remove once
    # #43 is merged
    (c, clow, chigh), = boot_ci(collect(d[:cos]))
    (m, mlow, mhigh), = boot_ci(collect(d[:mse]))
    DataFrame(cos=c, cos_low=clow, cos_high=chigh,
              mse=m, mse_low=mlow, mse_high=mhigh)
end

```

```{julia; label="recall-results"}

rec_params_good = @_ recalled_summaries |>
    @where(_, :Sσ .≈ 0.01) |>
    sort!(_, :cos, rev=true) |> 
    first(_, 1) 

# average over iterations
recalled_good = @_ rec_params_good |>
    join(recalled_all, _, on=[:α, :ρ, :Sσ]) |>
    @by(_, [:subjid1, :block, :rep, :rep_number, :respnr, :rad, :var, :x, :y, :x_resp, :y_resp],
        x_mod = mean(:x_mod), y_mod = mean(:y_mod)) 

recalled1 = @_ recalled_good |>
    @where(_, :subjid1 .== one_subj)

# baselines
baseline!(df) = @_ df |>
    @transform(_, gt_rho_avg = rho.(:x,:y) .> mean(rho.(:x, :y))) |>
    # @with(_, (hcat(:x,:y) .* ifelse.(:gt_rho_avg, -1, 1))')
    @transform(_, cos_center = 1 .- Distances.colwise(CosineDist(),
                                                      -hcat(:x, :y)', 
                                                      hcat(:x_resp.-:x, :y_resp.-:y)'),
                  cos_mean_rho = 1 .- Distances.colwise(CosineDist(),
                                                        (hcat(:x,:y) .* ifelse.(:gt_rho_avg, -1, 1))',
                                                        hcat(:x_resp.-:x, :y_resp.-:y)'),
                  cos_mod = 1 .- Distances.colwise(CosineDist(),
                                                   hcat(:x_mod.-:x, :y_mod.-:y)',
                                                   hcat(:x_resp.-:x, :y_resp.-:y)'))

baseline_good = baseline!(recalled_good)
bs_center, bs_avgrho = @_ baseline_good |>
    @where(_, (!isnan).(:cos_center)) |>    # there is one trial with zero deviation...
    @with(_, (mean(:cos_center), mean(:cos_mean_rho)))

# baseline: chance responding throughout the arena:
function discuniform(n)
    θs = rand(n)' .* 2π
    ρs = sqrt.(rand(n)')
    vcat(cos.(θs) .* ρs,
         sin.(θs) .* ρs)
end

cos_rand = let xy = hcat(recall.x, recall.y)', xyresp = hcat(recall.x_resp, recall.y_resp)'
    [mean(1 .- Distances.colwise(CosineDist(),
                                 discuniform(size(xy, 2)) .- xy,
                                 xyresp .- xy))
     for _ in 1:1000]
end

```

### Example

```{julia; label="example-subj-recall"; fig_cap="Subject 4's recalled locations (gray arrows, pointing from studied to recalled location) compared with model simulation (blue arrows; \$\\alpha=0.01, \\rho=0.9, \\sigma_x=0.1\$)"}
@df recalled1 arena(:x, :y, quiver=(:x_mod .- :x, :y_mod .- :y),
                    seriestype=:quiver, label="Model")
@df recalled1 quiver!(:x, :y, quiver=(:x_resp .- :x, :y_resp .- :y),
                      color=GrayA(0.0, 0.5), label="Behavior")
```

### Quantified

```{julia; label="cosinesim-by-param"; fig_cap="Mean cosine-similarity of model predicted and actual recall deviations across parameter values (ribbons show 95% bootstrapped CIs over model runs).  Gray lines show baselines: always deviate toward center, average radius, and center of true clusters"}

@df(@where(recalled_summaries, :Sσ .≈ 0.01),
    plot(:α, :cos, ribbon = (:cos .- :cos_low, :cos_high .- :cos), group=:ρ, xscale=:log10, 
         xlims=(10^-2.5, 10^1.5), ylims=(0, 0.12), seriestype=:line,
         xlabel="Eagerness to create new clusters (concentration \\alpha)",
         ylabel=("Cosine sim. with behavior"),
         legend=:bottomleft, legend_title="Stickiness \\rho", line=2))

baseline_x = [10^-2.1, 10^1.1]

function plot_baseline!(y, label)
    plot!(baseline_x, ones(2)*y, color=Gray(0.7), label="")
    annotate!(baseline_x[end]*1.05, y, label)
end

plot_baseline!(cosinesim(known_recalled),
               text("Known\nclusters", 10, RGB(Gray(0.7)), :left, :bottom))
plot_baseline!(bs_center,
               text("Center", 10, RGB(Gray(0.7)), :left, :bottom))
plot_baseline!(bs_avgrho,
               text("Mean rad.", 10, RGB(Gray(0.7)), :left, :top))

plot!(baseline_x, ones(2)*quantile(cos_rand, 0.025), fillrange=quantile(cos_rand, 0.975),
      fillalpha=0.1, fillcolor=GrayA(0., 0.1), primary=false, linealpha=0)
annotate!(baseline_x[end]*1.05, mean(cos_rand),
          text("Random\nresponse\n(95% CI)", 10, RGB(Gray(0.7)), :left, :top))

```

```{julia; label="cosinesim-by-param-subj-cis"}

# there's a lot of variability by subjects (and small $n$ so very low power)

summaries_by_subj = by(recalled_all, [:α, :ρ, :Sσ, :subjid1]) do d
    DataFrame(cos=cosinesim(d), mse=Experiments.mse(d,d))
end

summaries_by_subj_cis = by(summaries_by_subj,
                           [:α, :ρ, :Sσ],
                           :cos => x -> NamedTuple{(:mean, :low, :high)}(boot_ci(x)...))

# @_ summaries_by_subj_cis |>
#     @where(_, :Sσ .≈ 0.01) |>
#     @df plot!(:α, :mean, ribbon = (:mean .- :low, :high .- :mean), group=:ρ, xscale=:log10,
#               fillalpha=0.1, color=(1:3)', label=["" "" ""], ylims=(0, 0.18))

@_ summaries_by_subj_cis |>
    @where(_, :Sσ .≈ 0.01) |>
    @df plot!(:α, :low, group=:ρ, xscale=:log10,
              linestyle=:dash, color=(1:3)', label=["" "" ""], ylims=(0, 0.18))

@_ summaries_by_subj_cis |>
    @where(_, :Sσ .≈ 0.01) |>
    @df plot!(:α, :high, group=:ρ, xscale=:log10,
              linestyle=:dash, color=(1:3)', label=["" "" ""], ylims=(0, 0.18))

```

## Prediction

```{julia}

predicted_good = @_ predicted_all |>
    join(_, deletecols!(copy(rec_params_good), :Sσ), on=[:α, :ρ]) |>
    join(_, @select(recall, :subjid1, :block, :respnr, :x, :y), 
            on=[:subjid1, :block, :respnr]) |>
    sort!(_, (:subjid1, :respnr, :pred))

predicted1 = @where(predicted_good, :subjid1 .== 7)

prediction_deviations = @_ predicted_good |>
    @transform(_, resp_dev = rho.(:x.-:x_resp, :y.-:y_resp), 
                  mod_dev = mean.(pairwise.(Ref(Euclidean()), 
                                            transpose.(hcat.(:x, :y)),
                                            transpose.(:xys_mod))))

```

### Examples

```{julia; label="pred-examples"; fig_cap="Subject 7's (red points) and model's (gray regions) predictions about upcoming locations at various points throughout the experiment and various prediction horizons.  The white points show the last recalled location."}

white_to_black = cgrad([Gray(.9), Gray(0)])

function plot_prediction_task(xys_mod::Matrix, 
                              x::Float64, y::Float64, 
                              x_resp::Float64, y_resp::Float64)
    p = arena(xys_mod[:,1], xys_mod[:,2], seriestype=:histogram2d, color=white_to_black, 
              bins=range(-1.2, stop=1.2, length=50), lims=(-1.2, 1.2))
    scatter!(p, [x], [y], color=:white, markerstrokecolor=colorant"black")
    scatter!(p, [x_resp], [y_resp], color=:red)
    return p
end

function plot_prediction_task(d::AbstractDataFrame, args...; title=true, kwargs...)
    plots = let plots = [], title=title
        @byrow! d begin
            push!(plots, plot_prediction_task(:xys_mod, :x, :y, :x_resp, :y_resp))
            title!(plots[end], title ? "Trial $(:respnr) (+$(:pred))" : "")
        end
        plots
    end
    plot(plots..., args...; kwargs...)
end

# 23 (+1 and +10...back to center)
# trial 80 (+5 vs +10)
# 140 (+1 vs +10)

p1 = @_ predicted1 |>
    @where(_, :respnr .== 23) |>
    plot_prediction_task(_, layout=(2,1), title=true)

p2 = @_ predicted1 |>
    @where(_, :respnr .== 80) |>
    plot_prediction_task(_, layout=(2,1), title=true)

p3 = @_ predicted1 |>
    @where(_, :respnr .== 140) |>
    plot_prediction_task(_, layout=(2,1), title=true)

plot(p1, p2, p3, layout=(1,3), size=(900, 600))

```



### Quantified

```{julia; label="pred-dev"; fig_cap="Model predicted (\$\\alpha=0.01, \\rho=0.9\$) and actual deviations from last studied point for prediction task.  Small points show deviations of predictions for each trial, and large points show average deviations for each lag (1, 5, or 10 trials)."}

p = @df(prediction_deviations,
        scatter(:resp_dev, :mod_dev,
                xlabel="Predicted distance from last studied",
                ylabel="Model predicted", 
                legend_title = "Predictions for next", legend=:bottomright,
                group=:pred,
                smooth=true,
                markerstrokecolor=:white,
                markeralpha=.3,
                line=2,
                aspect_ratio = :equal,
                size=(500,250)))

@df(by(prediction_deviations, [:pred], :resp_dev=>mean, :mod_dev=>mean),
    scatter!(p,
             :resp_dev_mean, :mod_dev_mean,
             group=:pred,
             color = (1:3)',    # use the colors for groups 1:3
             markersize=8,
             label=""))

plot!(p, 0:.1:maximum(prediction_deviations[:mod_dev]), x->x, color=GrayA(0.2, 0.2), label="")

```

```julia



```
