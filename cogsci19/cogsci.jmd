---
title: "A Bayesian model of memory in a multi-context environment"
author:
- name: "Dave F. Kleinschmidt"
  email: "dave.kleinschmidt@rutgers.edu"
- name: "Pernille Hemmer"
  email: "pernille.hemmer@rutgers.edu"
affiliation:
- "Department of Psychology"
- "Rutgers New Brunswick"
- "Piscawtaway NJ, 08854"
bibliography: cogsci.bib
---


```{julia; echo=false; results="hide"}
using Revise

using Weave
Weave.set_chunk_defaults(Dict{Symbol,Any}(
    :results => "hidden",
    :echo => false
))

using 
    LinearAlgebra, 
    Random,
    Statistics

using 
    Plots,
    PlotThemes,
    StatPlots,
    RecipesBase,
    Colors, 
    Images,
    DataFrames,
    DataFramesMeta,
    Underscore,
    StatsBase,
    ConjugatePriors,
    Particles,
    Distances, 
    JuliennedArrays,
    Bootstrap

theme(:default, markerstrokecolor=:white)

flip(x::AbstractVector) = reshape(x, (1,:))

const It = Base.Iterators

Revise.track("../modeling.jl")
using .DotLearning
Revise.track("../experiments.jl")
using .Experiments
include("../plots.jl")

using JLD2
@load "../data/dots2014.jld2"
# load the prior _parameters_ to get around
@load "../prior_empirical_params.jld2"
prior_optimized = ConjugatePriors.NormalInverseWishart(μ, κ, Λ, ν)
import .Experiments: cosinesim
cosinesim(d) = cosinesim(d,d)

rho(args...) = sqrt(sum(args.^2))
```

```julia

@load "../results/run3-2019-01-07T21:23:16.243-recalled-predicted.jld2" recalled_all predicted_all

summaries_by_iter = by(recalled_all, [:α, :ρ, :Sσ, :iter]) do d
    DataFrame(cos=cosinesim(d), mse=Experiments.mse(d,d))
end

boot_ci(x, n=1000, cil=0.95) = confint(bootstrap(mean, x, BasicSampling(n)),
                                       BasicConfInt(cil))

recalled_summaries = by(summaries_by_iter, [:α, :ρ, :Sσ]) do d
    # collect only necessary here because of Bootstrap.jl#42, can remove once
    # #43 is merged
    (c, clow, chigh), = boot_ci(collect(d[:cos]))
    (m, mlow, mhigh), = boot_ci(collect(d[:mse]))
    DataFrame(cos=c, cos_low=clow, cos_high=chigh,
              mse=m, mse_low=mlow, mse_high=mhigh)
end

```


# Notes

When remembering a particular item people draw on the _context_ that item
occurred in as an additional source of information [e.g., @Orhan2013; @Qian2014;
@Huttenlocher1991].  For instance, @Huttenlocher1991 found that immediate
spatial recall of a location in a circular area is biased towards the average
radius of all locations in the experiment.  They proposed that memory for an
individual item's location is encoded at two levels: the item itself, and the
_category_ it was assigned to.  Building on this, @Robbins2014 found that when
people are exposed to an environment where there are multiple clusters

((( Bayesian models of memory in other domains )))

Here, we evaluate whether Bayesian non-parametric clustering can explain peoples
behavior when the context occurs over time.



Can model
this as a kind of Bayesian cue combination: combine uncertain memory trace
(likelihood) contextual information (prior) to infer the distribution of
properties of the recalled item (posterior).

Begs the question what _is_ context?  It's not handed down by god.  Can think of
this as another level of inference: infer jointly which context a particular
item belongs to, and the likely properties that item had.

We treat multi-context memory as a non-parametric clustering problem: during
encoding, people must infer the _context_ (cluster) that that item belongs to,
which could any of the contexts they have encountered thus far, or a new
context.



## Data

@Robbins2014 - immediate spatial recall task (get methods from Pernille)

## Modeling

Our model has three components.  First, we model how people infer the assignment
of stimuli to contexts as nonparametric Bayesian clustering, approximated with a
particle filter.  Second, we model encoding and recall of locations as Bayesian
cue combination with a prior from the context.  Third, we model subjects'
predictions about future locations via the posterior predictive distribution of
the context model.

### Context model

We modeled learners inferences about the underlying context on each trial as a
sequential Bayesian non-parametric clustering problem.  The goal of the learner
in this model is to infer the cluster assignment $z_i$ of observation $x_i$,
given the previous observations $x_{1:i-1}$ and their labels $z_{1:i-1}$:

$$p(z_i=j | x_{1:i}, z_{1:i-1}) \propto p(x_i | z_i=j, z_{1:i-1}, x_{1:i-1})
p(z_i=j | z_{1:i-1}) $$

The sequential prior $p(z_i=j | z_{1:i-1})$ is a "Hibachi Grill Process"
[@Qian2014], which is like the standard Chinese Restaurant Process (CRP) with an
added (constant) probability assigned to the previous state.  This corresponds
to the following generative model: with probability $0 < \rho < 1$ the last
state is picked, $j=z_{i-1}$, and with probability $1-\rho$ a component is
chosen from a Chinese Restaurant Process with concentration $\alpha$, which
assigns probability to each state proportional to the number of observations
assigned to it already[^counts], and creates a new state with probability
proportional to $\alpha > 0$.

[^counts]: One important difference from a standard CRP is that only non-sticky
    transitions count for the purposes of sampling new states from the CRP.

The likelihood
$p(x_i | z_i=j, z_{1:i-1}, x_{1:i-1}) = p(x_i | x_{\{k; z_k=j\}})$ is computed
by marginalizing over the mean and covariance of a multivariate normal
distribution given the data points previously assigned to that cluster and a
conjugate Normal-Inverse Wishart prior [@Gelman2003].  This has the advantage
that it only requires tracking the sufficient statistics of the previous
observations from the cluster (sample mean and covariance), and not the
individual observations.

### Inference: Sequential Monte Carlo

Instead of a standard batch inference technique, we use an online, Sequential
Monte Carlo/particle filter technique.  This method approximates the posterior
beliefs after $i-1$ observations $p(z_{1:i-1} | x_{1:i-1})$ as a weighted
population of $K$ particles, each of which is one possible value of the $i-1$
labels, denoted $z_{1:i-1}^{(k)}$.  This population of particles represents an
_importance sample_ from the posterior.  When a new observation $x_i$ comes in,
the population moves to target the updated posterior $p(z_{1:i} | x_{1:i})$.
There are many algorithms to do this, and the effectiveness of a particular
algorithm will depend on the problem.  We use the algorithm of @Chen2000 [as
described in @Fearnhead2004]: for each particle $k$, a state assignment is
sampled for $x_i$ according to $p(z_i | x_{1:i}, z^{(k)}_{1:i-1})$, and the
weight $w^{(k)}_i$ is updated by the ratio of
$$
\frac{\sum_j p((z_{1:i-1}^{(k)},j) | x_{1:i})}
     {p(z_{1:i-1}^{(k)} | x_{1:i-1})}
$$
to ensure that each particle's weight reflects it's ability to _predict_ the
point $x_i$, rather than just _explain_ it.  When too much of the total weight
for the population (constrained to sum to 1) is captured by a small number of
particles (measured by the ratio of the variance of the weights to their mean
being greater than $0.5$), a new population is resampled (with replacement) and
the weights are set to be uniform.

This is for two reasons.  First, because we wish to query the
model's beliefs about the current context at every point throughout the
experiment, an online approximation is much more computationally efficient.  A
batch algorithm like Gibbs sampling or Hamiltonian Monte Carlo requries one full
sweep through the data for each sample, which must be done independently for
each data point, so drawing $K$ samples for each of $N$ data points is
$O(KN^2)$.  A particle filter propogates uncertainty with a fixed population of
$K$ particles, updating each particle in parallel as each data point comes in,
meaning the complexity is only $O(KN)$.  This means it's possible to effectively
model longer experiments.

Second, an online learning algorithm better approximates _psychological_
constraints on learning, and in particular unlike batch MCMC algorithms does not
assume that learners can go back and revisit each observation and their
decisions about it.[^revise] This class of models thus provides a possible
bridge between computational and algorithmic level approaches to modeling
learning and memory [@Sanborn2010; @Kleinschmidt2018b].

[^revise]: These approaches also do not _preclude_ revising previous decisions,
    they just do not _require_ it.

### Encoding and recall

The noisy memory trace is modeled as a normal distribution centered at the
studied location $x$ with an isometric covariance matrix $\Sigma_x$, whose
diagonal elements are all equal to $\sigma^2_x$, which is a free parameter of
the model.  This noisy memory trace is combined with a _context prior_, which is
approximated by the population of particles.  Specifically, each particle $k$
represents one possible assignment of the observations $x_{1:i}$ to clusters
$z^{(k)}_{1:i}$.  We can thus model each particle's context as the expected mean
and covariance matrix for all the points that particle $k$ has assigned to the
same cluster as the studied point $z^{(k)}_i$:

$$
\mu^{(k)}_c, \Sigma^{(k)}_c = E(\mu, \Sigma)_{p(\mu, \Sigma | x_{1:i},
  z^{(k)}_{1:i})}
$$

Then the best guess of the studied location under particle $k$'s model of the
context is the combination of a normal likelihood (from the noisy trace of the
studied item) and a normal prior (from the context), which works out to be the
inverse variance-weighted average of the two means:

$$
\hat x^{(k)} = ({\Sigma^{(k)}_c}^{-1} + \Sigma_x^{-1})^{-1}
    ({\Sigma^{(k)}_c}^{-1} \mu^{(k)}_c + \Sigma_x^{-1} x)
$$

### Prediction

To model subjects predictions about future locations, we sample 100 locations
from the posterior predictive distribution of the population of particles.  To
sample one predicted location at a $n$ trials in the future, we sample a
particle from the population according to their weights, draw a sample of $n$
future states from that particle's Hibachi Grill Process, and then sample one
point from the posterior predictive distribution of the resulting cluster.  In
the case that the predicted cluster is a new cluster, we sample from the prior
predictive.

## Procedure

To evaluate this model, we simulated the data from @Robbins2014. 

The particle filter algorithm was implemented in Julia 1.0 [@Bezanson2017].

# Results

```{julia; }

# recall data from one subject
recall1 = @where(recall, :subjid1 .== 7)

rf = RecallFilter(FearnheadParticles(100,
                                     prior_optimized,
                                     StickyCRP(0.01, 0.9)),
                  Matrix(0.01I,2,2))
filter!(rf, extract_data(recall1, rf))
```

## Clustering

```{julia; label="assign-mats"; fig_cap="Assignment similarity matrices for one population of particles (left) and the experimenter-defined clusters (right)"}

filter_sim = assignment_similarity(rf)

true_clus_sim = @with(recall1, :block .== :block')

euc_dist = @_ recall1 |>
    @with(_, hcat(:x, :y)) |>
    Distances.pairwise(Euclidean(), _')

plot(plot(Gray.(filter_sim), title="Inferred clusters"),
     plot(Gray.(true_clus_sim), title="True blocks"),
     plot(Gray.(1 .- euc_dist ./ maximum(euc_dist)), title="Euclidean distance"),
     axis=false, aspect_ratio=:equal, layout=(1,3), size=(1200,400))

```

First, how well does this algorithm do at recovering the underlying cluster
structure?  This is not a straightforward question to answer: each particle in
the population represents a potentially different assignment of observations to
clusters, and the cluster indices used in one particle might not align with
those in another particle.  To get around this we look at the assignment
similarity matrix, which is an $N\times N$ matrix, where element $(i,j)$ is the
probability that trials $i$ and $j$ are assigned to the same cluster.  This
probability is calculated by averaging across all particles in the population
according to their weight.

@Fig:assign-mats shows the assignment similarty matrix for one subject, based on
a 100-particle filter with $\alpha=0.01, \rho=0.9$ (left), the true,
experimenter-defined block structure (middle), and pairwise Euclidean distance
(right).

## Recall

```{julia; label="recall-results"}

rec_params_good = @_ recalled_summaries |>
    @where(_, :Sσ .≈ 0.01) |>
    sort!(_, :cos, rev=true) |> 
    first(_, 1) 

# average over iterations
recalled_good = @_ rec_params_good |>
    join(recalled_all, _, on=[:α, :ρ, :Sσ]) |>
    @by(_, [:subjid1, :block, :rep, :rep_number, :respnr, :rad, :var, :x, :y, :x_resp, :y_resp],
        x_mod = mean(:x_mod), y_mod = mean(:y_mod)) 

recalled1 = @_ recalled_good |>
    @where(_, :subjid1 .== 7)

# baselines

known_recalled = by(recall, :subjid1) do d
    @_ KnownFilter(prior_optimized) |>
    RecallFilter(_, Matrix(0.01I,2,2)) |>
    filter!(_, extract_data(d, _)) |>
    DataFrame |>
    hcat(d, _) |>
    deletecols!(_, :subjid1) |>
    @transform(_, cosinesim = 1 .- Distances.colwise(CosineDist(), 
                                                   hcat(:x_mod.-:x, :y_mod.-:y)', 
                                                   hcat(:x_resp.-:x, :y_resp.-:y)'))
end

known_recalled1 = @where(known_recalled, :subjid1 .== 7)

baseline!(df) = @_ df |>
    @transform(_, gt_rho_avg = rho.(:x,:y) .> mean(rho.(:x, :y))) |>
    # @with(_, (hcat(:x,:y) .* ifelse.(:gt_rho_avg, -1, 1))')
    @transform(_, cos_center = 1 .- Distances.colwise(CosineDist(),
                                                      -hcat(:x, :y)', 
                                                      hcat(:x_resp.-:x, :y_resp.-:y)'),
                  cos_mean_rho = 1 .- Distances.colwise(CosineDist(),
                                                        (hcat(:x,:y) .* ifelse.(:gt_rho_avg, -1, 1))',
                                                        hcat(:x_resp.-:x, :y_resp.-:y)'),
                  cos_mod = 1 .- Distances.colwise(CosineDist(),
                                                   hcat(:x_mod.-:x, :y_mod.-:y)',
                                                   hcat(:x_resp.-:x, :y_resp.-:y)'))

baseline_good = baseline!(recalled_good)
bs_center, bs_avgrho = @_ baseline_good |>
    @where(_, (!isnan).(:cos_center)) |>    # there is one trial with zero deviation...
    @with(_, (mean(:cos_center), mean(:cos_mean_rho)))

```

```{julia; label="example-subj-recall"; fig_cap="One subject's recall data plus model predictions"}
@df recalled1 arena(:x, :y, quiver=(:x_resp .- :x, :y_resp .- :y), seriestype=:quiver)
@df recalled1 quiver!(:x, :y, quiver=(:x_mod .- :x, :y_mod .- :y), color=:black)
```

Next, we assess how well the inferred contexts can predict recall.  Figure
-@fig:example-subj-recall shows one subject's actual deviations from studied to
recalled locations (blue arrows) versus the model's predicted deviations (black
arrows).  To quantify goodness of fit, we use the cosine similarity of the
model's and subject's recall deviation (i.e., blue and black arrows in
@Fig:example-subj-recall).  We chose this metric because it is less sensitive to
large outlier responses than mean-squared error, and because approximations of
the likelihood of a subject's response given the model is highly sensitive to
free parameters and difficult to reliably estimate.  Moreover, the baseline
models we compare against also do not have straightforward likelihood models,
but they _do_ make straightforward predictions about the directions of recall
deviations.

```{julia; label="cosinesim-by-param"; fig_cap="Cosine-similarity of model predicted and actual recall deviations across parameter values.  Gray lines show baselines: always deviate toward center, average radius, and center of true clusters"}

@df(@where(recalled_summaries, :Sσ .≈ 0.01),
    plot(:α, :cos, ribbon = (:cos .- :cos_low, :cos_high .- :cos), group=:ρ, xscale=:log10, 
         xlims=(10^-2.5, 10^1.5), ylims=(0, 0.12), seriestype=:line,
         xlabel="Eagerness to create new clusters (DP concentration \\alpha)",
         ylabel=("Cosine sim. with behavior"),
         legend=:bottomright, legend_title="Stickiness \\rho", line=2))

baseline_x = [10^-2.1, 10^1.1]

function plot_baseline!(y, label)
    plot!(baseline_x, ones(2)*y, color=Gray(0.7), label="")
    annotate!(baseline_x[end]*1.05, y, label)
end

plot_baseline!(cosinesim(known_recalled),
               text("Known\nclusters", 10, RGB(Gray(0.7)), :left, :bottom))
plot_baseline!(bs_center,
               text("Center", 10, RGB(Gray(0.7)), :left, :bottom))
plot_baseline!(bs_avgrho,
               text("Mean rad.", 10, RGB(Gray(0.7)), :left, :top))

```

Figure -@fig:cosinesim-by-param shows the cosine similarity with of all
subjects' responses with the multi-context Bayesian model.  We compare the
model's performance against three baselines.  First, we compare it against a
"known clusters" model, which uses the true (experimenter defined) clusters with
the same Bayesian cue combination model of encoding and recall.  Second, we
compare it to two baselines based on previous literature on similar memory
tasks [@Huttenlocher1991]: one that always biases recall towards the center (the
average location of all trials), and one that baises recall towards the mean
radius.

First, at the whole range of parameters explored, the multi-context model
performs better than the center- or mean-radius-biased baselines.  Second,
except for low stickiness $\rho=0.1$, our model provides a better fit to human
behavior than the "known clusters" baseline, which differs from our model only
in that the true cluster labels are provided for each data point, rather than
being inferred.  This suggests that, at least according to the cosine similarity
metric, our context-inference model better captures how people combine
information about the current context during recall than the "ground truth"
clusters.



## Prediction

```{julia}

predicted_good = @_ predicted_all |>
    join(_, deletecols!(copy(rec_params_good), :Sσ), on=[:α, :ρ]) |>
    join(_, @select(recall, :subjid1, :block, :respnr, :x, :y), 
            on=[:subjid1, :block, :respnr]) |>
    sort!(_, (:subjid1, :respnr, :pred))

predicted1 = @where(predicted_good, :subjid1 .== 7)

prediction_deviations = @_ predicted_good |>
    @transform(_, resp_dev = rho.(:x.-:x_resp, :y.-:y_resp), 
                  mod_dev = mean.(pairwise.(Ref(Euclidean()), 
                                            transpose.(hcat.(:x, :y)),
                                            transpose.(:xys_mod))))

```

```{julia; label="pred-dev"; fig_cap="Model predicted ($\alpha=0.01, \rho=0.9$) and actual deviations from last studied point for prediction task.  Small points show deviations of predictions for each trial, and large points show average deviations for each lag (1, 5, or 10 trials)."}

p = @df(prediction_deviations,
        scatter(:resp_dev, :mod_dev,
                xlabel="Response deviation",
                ylabel="Model predicted", 
                legend_title = "Predictions for next", legend=:bottomright,
                group=:pred,
                smooth=true,
                markerstrokecolor=:white,
                markeralpha=.3,
                line=2,
                aspect_ratio = :equal,
                size=(500,250)))

@df(by(prediction_deviations, [:pred], :resp_dev=>mean, :mod_dev=>mean),
    scatter!(p,
             :resp_dev_mean, :mod_dev_mean,
             group=:pred,
             color = (1:3)',    # use the colors for groups 1:3
             markersize=8,
             label=""))

plot!(p, 0:.1:maximum(prediction_deviations[:mod_dev]), x->x, color=GrayA(0.2, 0.2), label="")

```

Our model also captures subjects' predictions about the location of upcoming
points at different delays.


# Conclusion
