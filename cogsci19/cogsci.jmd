---
title: "A Bayesian model of memory in a multi-context environment"
author:
- name: "Dave F. Kleinschmidt"
  email: "dave.kleinschmidt@rutgers.edu"
- name: "Pernille Hemmer"
  email: "pernille.hemmer@rutgers.edu"
affiliation:
- "Department of Psychology"
- "Rutgers New Brunswick"
- "Piscawtaway NJ, 08854"
bibliography: cogsci.bib
---


```{julia; echo=false; results="hide"}
using Revise

using Weave
Weave.set_chunk_defaults(Dict{Symbol,Any}(
    :results => "hidden",
    :echo => false
))

using 
    LinearAlgebra, 
    Random,
    Statistics

using 
    Plots,
    PlotThemes,
    StatPlots,
    RecipesBase,
    Colors, 
    Images,
    DataFrames,
    DataFramesMeta,
    Underscore,
    StatsBase,
    ConjugatePriors,
    Particles,
    Distances, 
    JuliennedArrays,
    Bootstrap

theme(:default, markerstrokecolor=:white)

flip(x::AbstractVector) = reshape(x, (1,:))

const It = Base.Iterators

Revise.track("../modeling.jl")
using .DotLearning
Revise.track("../experiments.jl")
using .Experiments
include("../plots.jl")

using JLD2
@load "../data/dots2014.jld2"
# load the prior _parameters_ to get around
@load "../prior_empirical_params.jld2"
prior_optimized = ConjugatePriors.NormalInverseWishart(μ, κ, Λ, ν)
import .Experiments: cosinesim
cosinesim(d) = cosinesim(d,d)

rho(args...) = sqrt(sum(args.^2))
```

```julia

@load "../results/run3-2019-01-07T21:23:16.243-recalled-predicted.jld2" recalled_all predicted_all

summaries_by_iter = by(recalled_all, [:α, :ρ, :Sσ, :iter]) do d
    DataFrame(cos=cosinesim(d), mse=Experiments.mse(d,d))
end

boot_ci(x, n=1000, cil=0.95) = confint(bootstrap(mean, x, BasicSampling(n)),
                                       BasicConfInt(cil))

recalled_summaries = by(summaries_by_iter, [:α, :ρ, :Sσ]) do d
    # collect only necessary here because of Bootstrap.jl#42, can remove once
    # #43 is merged
    (c, clow, chigh), = boot_ci(collect(d[:cos]))
    (m, mlow, mhigh), = boot_ci(collect(d[:mse]))
    DataFrame(cos=c, cos_low=clow, cos_high=chigh,
              mse=m, mse_low=mlow, mse_high=mhigh)
end

```


# Notes

When remembering a particular item people draw on the _context_ that item
occurred in as an additional source of information [e.g., @Orhan2013; @Qian2014;
@Huttenlocher1991].  For instance, @Huttenlocher1991 found that immediate
spatial recall of a location in a circular area is biased towards the average
radius of all locations in the experiment.  They proposed that memory for an
individual item's location is encoded at two levels: the item itself, and the
_category_ it was assigned to.  Building on this, @Robbins2014 found that when
people are exposed to an environment where there are multiple clusters

((( Bayesian models of memory in other domains )))

Here, we evaluate whether Bayesian non-parametric clustering can explain peoples
behavior when the context occurs over time.



Can model
this as a kind of Bayesian cue combination: combine uncertain memory trace
(likelihood) contextual information (prior) to infer the distribution of
properties of the recalled item (posterior).

Begs the question what _is_ context?  It's not handed down by god.  Can think of
this as another level of inference: infer jointly which context a particular
item belongs to, and the likely properties that item had.

We treat multi-context memory as a non-parametric clustering problem: during
encoding, people must infer the _context_ (cluster) that that item belongs to,
which could any of the contexts they have encountered thus far, or a new
context.



## Data

@Robbins2014 - immediate spatial recall task (get methods from Pernille)

## Modeling

Our model has three components.  First, we model how people infer the assignment
of stimuli to contexts as nonparametric Bayesian clustering, approximated with a
particle filter.  Second, we model encoding and recall of locations as Bayesian
cue combination with a prior from the context.  Third, we model subjects'
predictions about future locations via the posterior predictive distribution of
the context model.

### Context model

We modeled learners inferences about the underlying context on each trial as a
sequential Bayesian non-parametric clustering problem.  The goal of the learner
in this model is to infer the cluster assignment $z_i$ of observation $x_i$,
given the previous observations $x_{1:i-1}$ and their labels $z_{1:i-1}$:

$$p(z_i=j | x_{1:i}, z_{1:i-1}) \propto p(x_i | z_i=j, z_{1:i-1}, x_{1:i-1})
p(z_i=j | z_{1:i-1}) $$

The sequential prior $p(z_i=j | z_{1:i-1})$ is a "Hibachi Grill Process"
[@Qian2014], which is like the standard Chinese Restaurant Process (CRP) with an
added (constant) probability assigned to the previous state.  This corresponds
to the following generative model: with probability $0 < \rho < 1$ the last
state is picked, $j=z_{i-1}$, and with probability $1-\rho$ a component is
chosen from a Chinese Restaurant Process with concentration $\alpha$, which
assigns probability to each state proportional to the number of observations
assigned to it already[^counts], and creates a new state with probability
proportional to $\alpha > 0$.

[^counts]: One important difference from a standard CRP is that only non-sticky
    transitions count for the purposes of sampling new states from the CRP.

The likelihood
$p(x_i | z_i=j, z_{1:i-1}, x_{1:i-1}) = p(x_i | x_{\{k; z_k=j\}})$ is computed
by marginalizing over the mean and covariance of a multivariate normal
distribution given the data points previously assigned to that cluster and a
conjugate Normal-Inverse Wishart prior [@Gelman2003].  This has the advantage
that it only requires tracking the sufficient statistics of the previous
observations from the cluster (sample mean and covariance), and not the
individual observations.

### Inference: Sequential Monte Carlo

Instead of a standard batch inference technique, we use an online, Sequential
Monte Carlo/particle filter technique.  This method approximates the posterior
beliefs after $i-1$ observations $p(z_{1:i-1} | x_{1:i-1})$ as a weighted
population of $K$ particles, each of which is one possible value of the $i-1$
labels, denoted $z_{1:i-1}^{(k)}$.  This population of particles represents an
_importance sample_ from the posterior.  When a new observation $x_i$ comes in,
the population moves to target the updated posterior $p(z_{1:i} | x_{1:i})$.
There are many algorithms to do this, and the effectiveness of a particular
algorithm will depend on the problem.  We use the algorithm of @Chen2000 [as
described in @Fearnhead2004]: for each particle $k$, a state assignment is
sampled for $x_i$ according to $p(z_i | x_{1:i}, z^{(k)}_{1:i-1})$, and the
weight $w^{(k)}_i$ is updated by the ratio of
$$
\frac{\sum_j p((z_{1:i-1}^{(k)},j) | x_{1:i})}
     {p(z_{1:i-1}^{(k)} | x_{1:i-1})}
$$
to ensure that each particle's weight reflects it's ability to _predict_ the
point $x_i$, rather than just _explain_ it.  When too much of the total weight
for the population (constrained to sum to 1) is captured by a small number of
particles (measured by the ratio of the variance of the weights to their mean
being greater than $0.5$), a new population is resampled (with replacement) and
the weights are set to be uniform.

This is for two reasons.  First, because we wish to query the
model's beliefs about the current context at every point throughout the
experiment, an online approximation is much more computationally efficient.  A
batch algorithm like Gibbs sampling or Hamiltonian Monte Carlo requries one full
sweep through the data for each sample, which must be done independently for
each data point, so drawing $K$ samples for each of $N$ data points is
$O(KN^2)$.  A particle filter propogates uncertainty with a fixed population of
$K$ particles, updating each particle in parallel as each data point comes in,
meaning the complexity is only $O(KN)$.  This means it's possible to effectively
model longer experiments.

Second, an online learning algorithm better approximates _psychological_
constraints on learning, and in particular unlike batch MCMC algorithms does not
assume that learners can go back and revisit each observation and their
decisions about it.[^revise] This class of models thus provides a possible
bridge between computational and algorithmic level approaches to modeling
learning and memory [@Sanborn2010; @Kleinschmidt2018b].

[^revise]: These approaches also do not _preclude_ revising previous decisions,
    they just do not _require_ it.

### Encoding and recall

The noisy memory trace is modeled as a normal distribution centered at the
studied location $x$ with an isometric covariance matrix $\Sigma_x$, whose
diagonal elements are all equal to $\sigma^2_x$, which is a free parameter of
the model.  This noisy memory trace is combined with a _context prior_, which is
approximated by the population of particles.  Specifically, each particle $k$
represents one possible assignment of the observations $x_{1:i}$ to clusters
$z^{(k)}_{1:i}$.  We can thus model each particle's context as the expected mean
and covariance matrix for all the points that particle $k$ has assigned to the
same cluster as the studied point $z^{(k)}_i$:

$$
\mu^{(k)}_c, \Sigma^{(k)}_c = E(\mu, \Sigma)_{p(\mu, \Sigma | x_{1:i},
  z^{(k)}_{1:i})}
$$

Then the best guess of the studied location under particle $k$'s model of the
context is the combination of a normal likelihood (from the noisy trace of the
studied item) and a normal prior (from the context), which works out to be the
inverse variance-weighted average of the two means:

$$
\hat x^{(k)} = ({\Sigma^{(k)}_c}^{-1} + \Sigma_x^{-1})^{-1}
    ({\Sigma^{(k)}_c}^{-1} \mu^{(k)}_c + \Sigma_x^{-1} x)
$$

### Prediction

To model subjects predictions about future locations, we sample 100 locations
from the posterior predictive distribution of the population of particles.  To
sample one predicted location at a $n$ trials in the future, we sample a
particle from the population according to their weights, draw a sample of $n$
future states from that particle's Hibachi Grill Process, and then sample one
point from the posterior predictive distribution of the resulting cluster.  In
the case that the predicted cluster is a new cluster, we sample from the prior
predictive.

## Procedure

To evaluate this model, we simulated the data from @Robbins2014. 

The particle filter algorithm was implemented in Julia 1.0 [@Bezanson2017].

# Results

```{julia; }

# recall data from one subject
recall1 = @where(recall, :subjid1 .== 7)

rf = RecallFilter(ChenLiuParticles(100,
                                   prior_optimized,
                                   StickyCRP(0.01, 0.9)),
                  Matrix(0.01I,2,2))
filter!(rf, extract_data(recall1, rf))

recalled1 = hcat(recall1, DataFrame(rf))

```

## Clustering

```{julia; label="assign-mats"; fig_cap="Assignment similarity matrices for one population of particles (left) and the experimenter-defined clusters (right)"}

plot(plot(show_assignment_similarity(rf), title="Inferred clusters"),
     plot(Gray.(@with(recall1, :block .== :block')), title="True blocks"),
     axis=false, aspect_ratio=:equal, layout=(1,2), size=(800,400))

```

First, how well does this algorithm do at recovering the underlying cluster
structure?  This is not a straightforward question to answer: each particle in
the population represents a potentially different assignment of observations to
clusters, and the cluster indices used in one particle might not align with
those in another particle.  To get around this we look at the assignment
similarity matrix, which is an $N\times N$ matrix, where element $(i,j)$ is the
probability that trials $i$ and $j$ are assigned to the same cluster.  This
probability is calculated by averaging across all particles in the population
according to their weight.



## Recall

Next, we assess how well the inferred contexts can predict recall.  We compare
the model's performance against three baselines.  First, we compare it against a
"known clusters" model, which uses the true (experimenter defined) clusters with
the same Bayesian cue combination model of encoding and recall.  Second, we
compare it to two baselines based on previous literature: one that always biases
recall towards the center (the average location of all trials), and one that
baises recall towards the mean radius [as in @Huttenlocher1991].  The metric we
use for comparison is the cosine similarity of the recall deviation (the vector
from the studied location to the recalled location).  We chose this metric
because it relies on fewer free parameters than directly estimating the
likelihood of the recalled location given each of the models, and is less
sensitive to outliers than the actual distance.

## Prediction

# Conclusion
