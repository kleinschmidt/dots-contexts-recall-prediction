@ARTICLE{Bezanson2017,
  ABSTRACT = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast and questions notions generally held to be “laws of nature" by practitioners of numerical computing: $\backslash$beginlist $\backslash$item High-level dynamic programs have to be slow. $\backslash$item One must prototype in one language and then rewrite in another language for speed or deployment. $\backslash$item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. $\backslash$endlist We introduce the Julia programming language and its design---a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that one can achieve machine performance without sacrificing human convenience.},
  AUTHOR = {Bezanson, J. and Edelman, A. and Karpinski, S. and Shah, V.},
  DATE = {2017-01-01},
  DOI = {10.1137/141000671},
  FILE = {/home/dave/Zotero/storage/UHRM6YJT/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf;/home/dave/Zotero/storage/9FRJEQ22/141000671.html},
  ISSN = {0036-1445},
  JOURNALTITLE = {SIAM Review},
  NUMBER = {1},
  PAGES = {65--98},
  SHORTJOURNAL = {SIAM Rev.},
  SHORTTITLE = {Julia},
  TITLE = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  VOLUME = {59},
}

@ARTICLE{Chen2000,
  ABSTRACT = {In treating dynamic systems, sequential Monte Carlo methods use discrete samples to represent a complicated probability distribution and use rejection sampling, importance sampling and weighted resampling to complete the on-line 'filtering' task. We propose a special sequential Monte Carlo method, the mixture Kalman filter, which uses a random mixture of the Gaussian distributions to approximate a target distribution. It is designed for on-line estimation and prediction of conditional and partial conditional dynamic linear models, which are themselves a class of widely used non-linear systems and also serve to approximate many others. Compared with a few available filtering methods including Monte Carlo methods, the gain in efficiency that is provided by the mixture Kalman filter can be very substantial. Another contribution of the paper is the formulation of many non-linear systems into conditional or partial conditional linear form, to which the mixture Kalman filter can be applied. Examples in target tracking and digital communications are given to demonstrate the procedures proposed.},
  AUTHOR = {Chen, Rong and Liu, Jun S.},
  DATE = {2000},
  EPRINT = {2680693},
  EPRINTTYPE = {jstor},
  FILE = {/home/dave/Zotero/storage/ZS5PV2EV/Chen and Liu - 2000 - Mixture Kalman Filters.pdf},
  ISSN = {1369-7412},
  JOURNALTITLE = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  NUMBER = {3},
  PAGES = {493--508},
  TITLE = {Mixture {{Kalman Filters}}},
  VOLUME = {62},
}

@ARTICLE{DuBrow2017,
  ABSTRACT = {Theories of episodic memory have generally proposed that individual memory traces are linked together by a representation of context that drifts slowly over time. Recent data challenge the notion that contextual drift is always slow and passive. In particular, ...},
  AUTHOR = {DuBrow, S. and Rouhani, N. and Niv, Y. and Norman, K. A.},
  DATE = {2017-10},
  DOI = {10.1016/j.cobeha.2017.08.003},
  EPRINT = {29335678},
  EPRINTTYPE = {pmid},
  FILE = {/home/dave/Zotero/storage/FFKMBXA2/DuBrow et al. - 2017 - Does mental context drift or shift.pdf;/home/dave/Zotero/storage/JJMCUAZ8/pmc5766042.html},
  ISSN = {2352-1546},
  JOURNALTITLE = {Current opinion in behavioral sciences},
  LANGID = {english},
  PAGES = {141--146},
  SHORTJOURNAL = {Curr Opin Behav Sci},
  TITLE = {Does Mental Context Drift or Shift?},
  VOLUME = {17},
}

@ARTICLE{Fearnhead2004,
  AUTHOR = {Fearnhead, Paul},
  DATE = {2004-01},
  DOI = {10.1023/B:STCO.0000009418.04621.cd},
  FILE = {/home/dave/Zotero/storage/IUA3THW9/Fearnhead - 2004 - Particle filters for mixture models with an unknown number of components.pdf},
  JOURNALTITLE = {Statistics and Computing},
  KEYWORDS = {particle filters,mcmc,dirichlet process,gaussian mixture models,gibbs sampling},
  NUMBER = {1},
  PAGES = {11--21},
  TITLE = {Particle Filters for Mixture Models with an Unknown Number of Components},
  VOLUME = {14},
}

@BOOK{Gelman2003,
  AUTHOR = {Gelman, Andrew and Carlin, J B and Stern, H S and Rubin, Donald B.},
  PUBLISHER = {Taylor \& Francis},
  DATE = {2003},
  EDITION = {Second},
  ISBN = {978-1-4200-5729-4},
  SERIES = {Chapman \& {{Hall}}/{{CRC Texts}} in {{Statistical Science}}},
  TITLE = {Bayesian {{Data Analysis}}},
}

@ARTICLE{Huttenlocher1991,
  AUTHOR = {Huttenlocher, Janellen and Hedges, Larry V. and Duncan, Susan},
  DATE = {1991},
  DOI = {10.1037/0033-295X.98.3.352},
  FILE = {/home/dave/Zotero/storage/57KIX338/Huttenlocher et al. - 1991 - Categories and particulars Prototype effects in e.pdf},
  ISSN = {1939-1471, 0033-295X},
  JOURNALTITLE = {Psychological Review},
  LANGID = {english},
  NUMBER = {3},
  PAGES = {352--376},
  SHORTTITLE = {Categories and Particulars},
  TITLE = {Categories and Particulars: {{Prototype}} Effects in Estimating Spatial Location.},
  VOLUME = {98},
}

@INPROCEEDINGS{Kleinschmidt2018b,
  ABSTRACT = {Human infants have the remarkable ability to learn any human language. One proposed mechanism for this ability is distributional learning, where learners infer the underlying cluster structure from unlabeled input. Computational models of distributional learning have historically been principled but psychologically-implausible computational-level models, or ad hoc but psychologically plausible algorithmic-level models. Approximate rational models like particle ﬁlters can potentially bridge this divide, and allow principled, but psychologically plausible models of distributional learning to be speciﬁed and evaluated. As a proof of concept, I evaluate one such particle ﬁlter model, applied to learning English voicing categories from distributions of voice-onset times (VOTs). I ﬁnd that this model learns well, but behaves somewhat differently from the standard, unconstrained Gibbs sampler implementation of the underlying rational model.},
  AUTHOR = {Kleinschmidt, Dave F},
  EDITOR = {Rogers, Timothy T and Rau, X and Zhu, X and Kalish, Chuck},
  LOCATION = {Austin, TX},
  PUBLISHER = {Cognitive Science Society},
  BOOKTITLE = {Proceedings of the 40th {{Annual Conference}} of the {{Cognitive Science Society}}},
  DATE = {2018},
  DOI = {10.31234/osf.io/dymc8},
  FILE = {/home/dave/Zotero/storage/2LIMZQD8/Kleinschmidt - Learning distributions as they come Particle filt.pdf},
  LANGID = {english},
  PAGES = {1933--1938},
  SHORTTITLE = {Learning Distributions as They Come},
  TITLE = {Learning Distributions as They Come: {{Particle}} Filter Models for Online Distributional Learning of Phonetic Categories},
}

@ARTICLE{Orhan2013,
  AUTHOR = {Orhan, A. Emin and Jacobs, Robert A.},
  DATE = {2013},
  DOI = {10.1037/a0031541},
  FILE = {/home/dave/Zotero/storage/PMS3G6EA/Orhan and Jacobs - A Probabilistic Clustering Theory of the Organizat.pdf},
  ISSN = {1939-1471, 0033-295X},
  JOURNALTITLE = {Psychological Review},
  LANGID = {english},
  NUMBER = {2},
  PAGES = {297--328},
  TITLE = {A Probabilistic Clustering Theory of the Organization of Visual Short-Term Memory.},
  VOLUME = {120},
}

@ARTICLE{Pastell2017,
  AUTHOR = {Pastell, Matti},
  DATE = {2017-03-22},
  DOI = {10.21105/joss.00204},
  FILE = {/home/dave/Zotero/storage/WSBEP5BY/Pastell - 2017 - Weave.jl Scientific Reports Using Julia.pdf},
  ISSN = {2475-9066},
  JOURNALTITLE = {The Journal of Open Source Software},
  NUMBER = {11},
  PAGES = {204},
  SHORTTITLE = {Weave.Jl},
  TITLE = {Weave.Jl: {{Scientific Reports Using Julia}}},
  VOLUME = {2},
}

@ARTICLE{Qian2014,
  AUTHOR = {Qian, Ting and Aslin, Richard N},
  DATE = {2014},
  DOI = {10.1073/pnas.1416109111},
  FILE = {/home/dave/Zotero/storage/7D4PSEEK/Qian and Aslin - 2014 supplemental - Learning bundles of stimuli renders stimulus order.pdf;/home/dave/Zotero/storage/VRN98GBB/Qian, Aslin - 2014 - Learning bundles of stimuli renders stimulus order as a cue, not a confound.pdf},
  JOURNALTITLE = {Proceedings of the National Academy of Sciences},
  NUMBER = {40},
  PAGES = {14400--14405},
  TITLE = {Learning Bundles of Stimuli Renders Stimulus Order as a Cue, Not a Confound},
  VOLUME = {111},
}

@INPROCEEDINGS{Robbins2014,
  ABSTRACT = {Beliefs are a fundamental component of our daily decisions, and as such, beliefs about our health have a huge impact on our health behaviors. Poor medication adherence is a welldocumented problem and while it has been extensively researched, it has yet to be addressed using a Bayesian framework. This study aims to use a mixture model to understand belief updating as it affects decision making. Using an established experimental paradigm in categorical perception, we test memory and prediction in order to establish a model that can explain human belief updating. Results indicate that a mixture model provides a good explanation of participant behavior in this paradigm.},
  AUTHOR = {Robbins, Talia and Hemmer, Pernille and Tang, Yubei},
  LOCATION = {Quebec City},
  PUBLISHER = {Cognitive Science Society},
  BOOKTITLE = {Proceedings of the 36th {{Annual Conference}} of the {{Cognitive Science Society}}},
  DATE = {2014},
  FILE = {/home/dave/Zotero/storage/8YVQ79DA/Robbins et al. - Bayesian Updating A Framework for Understanding M.pdf},
  LANGID = {english},
  PAGES = {6},
  TITLE = {Bayesian {{Updating}}: {{A Framework}} for {{Understanding Medical Decision Making}}},
}

@ARTICLE{Roediger1995,
  ABSTRACT = {Two experiments (modeled after J. Deese's 1959 study) revealed remarkable levels of false recall and false recognition in a list learning paradigm. In Exp 1, Ss studied lists of 12 words (e.g., bed, rest, awake); each list was composed of associates of 1 nonpresented word (e.g., sleep). On immediate free recall tests, the nonpresented associates were recalled 40\% of the time and were later recognized with high confidence. In Exp 2, a false recall rate of 55\% was obtained with an expanded set of lists, and on a later recognition test, Ss produced false alarms to these items at a rate comparable to the hit rate. The act of recall enhanced later remembering of both studied and nonstudied material. The results reveal a powerful illusion of memory: People remember events that never happened. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  AUTHOR = {Roediger, Henry L. and McDermott, Kathleen B.},
  DATE = {1995},
  DOI = {10.1037/0278-7393.21.4.803},
  FILE = {/home/dave/Zotero/storage/2PTIZMPR/Roediger and McDermott - 1995 - Creating false memories Remembering words not pre.pdf;/home/dave/Zotero/storage/DRMLSUZM/1995-42833-001.html},
  ISSN = {1939-1285(Electronic),0278-7393(Print)},
  JOURNALTITLE = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  KEYWORDS = {Experimental Replication,Free Recall,Memory,Word Recognition},
  NUMBER = {4},
  PAGES = {803--814},
  SHORTTITLE = {Creating False Memories},
  TITLE = {Creating False Memories: {{Remembering}} Words Not Presented in Lists},
  VOLUME = {21},
}

@ARTICLE{Sanborn2010,
  ABSTRACT = {Rational models of cognition typically consider the abstract computational problems posed by the environment, assuming that people are capable of optimally solving those problems. This differs from more traditional formal models of cognition, which focus on the psychological processes responsible for behavior. A basic challenge for rational models is thus explaining how optimal solutions can be approximated by psychological processes. We outline a general strategy for answering this question, namely to explore the psychological plausibility of approximation algorithms developed in computer science and statistics. In particular, we argue that Monte Carlo methods provide a source of rational process models that connect optimal solutions to psychological processes. We support this argument through a detailed example, applying this approach to Anderson's (1990, 1991) rational model of categorization (RMC), which involves a particularly challenging computational problem. Drawing on a connection between the RMC and ideas from nonparametric Bayesian statistics, we propose 2 alternative algorithms for approximate inference in this model. The algorithms we consider include Gibbs sampling, a procedure appropriate when all stimuli are presented simultaneously, and particle filters, which sequentially approximate the posterior distribution with a small number of samples that are updated as new data become available. Applying these algorithms to several existing datasets shows that a particle filter with a single particle provides a good description of human inferences. (PsycINFO Database Record (c) 2010 APA, all rights reserved).},
  AUTHOR = {Sanborn, Adam N and Griffiths, Thomas L and Navarro, Daniel J},
  DATE = {2010-10},
  DOI = {10.1037/a0020511},
  EPRINT = {21038975},
  EPRINTTYPE = {pubmed},
  FILE = {/home/dave/Zotero/storage/JJRSXIMU/Sanborn, Griffiths, Navarro - 2010 - Rational approximations to rational models Alternative algorithms for category learning.pdf},
  JOURNALTITLE = {Psychological Review},
  KEYWORDS = {categorization,rational models,1990,aim to explain human,anderson,are posed by our,behavior as an optimal,chater,environment,oaks-,problems that,rational approximations,rational models of cognition,solution to the computational,thought and},
  NUMBER = {4},
  PAGES = {1144--67},
  TITLE = {Rational Approximations to Rational Models: {{Alternative}} Algorithms for Category Learning.},
  VOLUME = {117},
}

@ARTICLE{Schulz2018,
  ABSTRACT = {$<$p$>$How do humans search for rewards? This question is commonly studied using multi-armed bandit tasks, which require participants to trade off exploration and exploitation. Standard multi-armed bandits assume that each option has an independent reward distribution. However, learning about options independently is unrealistic, since in the real world options often share an underlying structure. We introduce a class of structured bandit tasks, which we use to probe how generalization guides exploration. In a structured multi-armed bandit, options have a correlation structure dictated by a latent function. We focus on bandits in which rewards are linear functions of an option9s spatial position. Across 5 experiments, we find evidence that participants utilize functional structure to guide their exploration, and also exhibit a learning-to-learn effect across rounds, becoming progressively faster at identifying the latent function. The experiments rule out several heuristic explanations, and show that the same findings obtain with non-linear functions. Comparing several models of learning and decision making, we find that the best model of human behavior in our tasks combines three computational mechanisms: (1) function learning, (2) clustering of reward distributions across rounds, and (3) uncertainty-guided exploration. Our results suggest that human reinforcement learning can utilize latent structure in sophisticated ways to improve efficiency.$<$/p$>$},
  AUTHOR = {Schulz, Eric and Franklin, Nicholas T. and Gershman, Samuel J.},
  DATE = {2018-12-29},
  DOI = {10.1101/432534},
  FILE = {/home/dave/Zotero/storage/ZQ264M2I/Schulz et al. - 2018 - Finding structure in multi-armed bandits.pdf;/home/dave/Zotero/storage/V79VWXYQ/432534v3.html},
  JOURNALTITLE = {bioRxiv},
  LANGID = {english},
  PAGES = {432534},
  TITLE = {Finding Structure in Multi-Armed Bandits},
}

