
```julia
using Pkg
Pkg.activate(".")

using Revise

using 
    LinearAlgebra, 
    Random,
    Statistics

using 
    Plots,
    PlotThemes,
    StatPlots,
    RecipesBase,
    Colors, 
    Images,
    DataFrames,
    DataFramesMeta,
    Underscore,
    StatsBase,
    ConjugatePriors,
    Particles,
    Distances, 
    JuliennedArrays

theme(:default, markerstrokecolor=:white)

flip(x::AbstractVector) = reshape(x, (1,:))

const It = Base.Iterators

Revise.track("modeling.jl")
using .DotLearning
Revise.track("experiments.jl")
using .Experiments
include("plots.jl")

using JLD2
using Compat
@load "data/dots2014.jld2"
# load the prior _parameters_ to get around
```

```julia
@load "prior_empirical_params.jld2"
prior_optimized = ConjugatePriors.NormalInverseWishart(μ, κ, Λ, ν)
```

```julia
@load "results/run3-2019-01-07T21:23:16.243.jld2" results expts

rho(args...) = sqrt(sum(args.^2))
```

<div class="intro">

<p>in the real world judgements are made in</p>
<p class="big">context</p>
<p class="fragment">which provides <em>useful information</em></p>
</div>

```julia
arena()
```

```julia
x, y = -0.3, 0.4
arena([x], [y], color="black")
```

```julia
plot(Gray.(rand(Bool, 100,100)), axis=false, lims=(0,100), aspect_ratio=:equal)
```

```julia
arena([x], [y], color=:white, markerstrokecolor=:black, markersize=5)
annotate!(0,0, text("?", 32))
```

Now you need to reconstruct the location of this dot from your memory, which has some uncertainty associated with it.

```julia
arena(randn(200).*0.2, randn(200).*0.2, color=:black, markeralpha=0.25)
scatter!([x], [y], color=:white, markerstrokecolor=:black, markersize=5)
```

```julia
quiver!([x], [y], quiver=(-[x*0.3], -[y.*0.3]), color=:black)
```

```julia
Random.seed!(2)
θ = rand(200) * 2π
ρ = randn(200) .* .05 .+ 0.85

arena(cos.(θ).*ρ, sin.(θ).*ρ, markeralpha=0.25, color=:black)
scatter!([x], [y], color=:white, markerstrokecolor=:black, markersize=5)


quiver!([x], [y], quiver=([x*.3], [y*.3]), color=:black)
```

<div class="intro">

<p>but what is a</p>
<p class="big">context?</p>
<p>and how do you <em>know?</em></p>
</div>

begs the question, what constitutes a context? and how is an agent supposed to know?

there are of course a lot of different sources of information about the _context_.  recall Jennifer Truebloods work, where context comes from available alternatives in a decision making task.  another important source of information about context is....history

```julia
recall1 = @where(recall, :subjid1 .== 7)
```

# history provides _context_

While it might look like there's a just a single context here, and teh dots appear randomly around the arena...

```julia
r1_shuffled = view(recall, randperm(180) .+ 20, :)

p = arena([], [], markeralpha=0.25, color=:black, lims=(-1.1,1.1))
anim = @animate for (x,y) in @_ zip(r1_shuffled[:x], r1_shuffled[:y])
    push!(p, x,y)
end

gif(anim, "figures/shuffled.gif", fps=5)
```

# history provides _context_

...if the same dots appear in a **different order** it's clear that there's actually a number of _different_ contexts, or clusters.

```julia
p = arena([], [], markeralpha=0.25, color=:black, lims=(-1.1,1.1))
anim = @animate for (x,y) in @_ zip(recall[:x], recall[:y]) |> It.drop(_, 20) |> It.take(_, 180)
    push!(p, x,y)
end

gif(anim, "figures/clustered.gif", fps=5)
```

# _Behavior_

In a structured environment recall is _biased_ towards clusters <span class="citation">[Robbins, Hemmer, and Tang, <em>CogSci2014</em>]</span>

...and in fact, in such structured environments, recall is **biased towards clusters!** above and beyond the overall context

```julia
known_recalled = by(recall, :subjid1) do d
    @_ KnownFilter(prior_optimized) |>
    RecallFilter(_, Matrix(0.01I,2,2)) |>
    filter!(_, extract_data(d, _)) |>
    DataFrame |>
    hcat(d, _) |>
    deletecols!(_, :subjid1) |>
    @transform(_, cosinesim = 1 .- Distances.colwise(CosineDist(), 
                                                   hcat(:x_mod.-:x, :y_mod.-:y)', 
                                                   hcat(:x_resp.-:x, :y_resp.-:y)'))
end

known_recalled1 = @where(known_recalled, :subjid1 .== 7)
```

```julia
arena(lims=(-1,1))
@_ known_recalled1 |>
    @by(_, :block, x_clus = mean(:x), y_clus = mean(:y)) |>
    @df(_, scatter!(:x_clus, :y_clus, color=:red, seriestype=:scatter, markerstrokecolor=:white))
@df recall1 quiver!(:x, :y, quiver=(:x_resp.-:x, :y_resp.-:y), color=:black, seriestype=:quiver, lims=(-1,1))
```

# _Behavior_ + <span style="color:red">cluster bias</span>

In a structured environment recall is _biased_ towards clusters <span class="citation">[Robbins, Hemmer, and Tang, <em>CogSci2014</em>]</span>

```julia
@df known_recalled1 arena(:x, :y, quiver=(:x_mod.-:x, :y_mod.-:y), seriestype=:quiver, color=:red)
```

```julia
arena(lims=(-1,1))
@_ known_recalled1 |>
    @by(_, :block, x_clus = mean(:x), y_clus = mean(:y)) |>
    @df(_, scatter!(:x_clus, :y_clus, color=:red, seriestype=:scatter, markerstrokecolor=:white))
@df known_recalled1 quiver!(:x, :y, quiver=(:x_resp.-:x, :y_resp.-:y), color=GrayA(0.0, 0.5))
@df known_recalled1 quiver!(:x, :y, quiver=(:x_mod.-:x, :y_mod.-:y), color=RGBA(1, 0, 0, 0.5))
```

# _approach_: bounded rationality

## _Computational-level_: Dirichlet Process mixture model

* _Infer_ how points $x_t$ are assigned $z_t$
    * $p(z_1, \ldots, z_T | x_1, \ldots, x_T) \propto p(x_1, \ldots, x_T | z_1, \ldots, z_T) p(z_1, \ldots, z_T)$
* Prior: "sticky" CRP $p(z_t = j | z_{1\ldots t-1}) \propto N_j (\times \frac{\rho}{1-\rho}$ if $z_{t-1}=j)$
    * $N_j = \alpha$ for all new $j$.
    * Prefer **small number** of contexts
    * Allow for up to $T$ (one per point)
* Likelihood: $p(x_t | z_t, z_{1:t-1}, x_{1:t-1})$
    * Prefer **compact clusters**

<h2 class="old"><em>Algorithmic-level</em></h2>

```julia
rf = RecallFilter(ChenLiuParticles(100, prior_optimized, StickyCRP(0.01, 0.9)), Matrix(0.01I,2,2))
filter!(rf, extract_data(recall1, rf))

recalled1 = hcat(recall1, DataFrame(rf))

rf_crp = RecallFilter(ChenLiuParticles(100, prior_optimized, ChineseRestaurantProcess(0.01)), Matrix(0.01I,2,2))
filter!(rf_crp, extract_data(recall1, rf_crp))
crp_recalled1 = hcat(recall1, DataFrame(rf_crp))

sort!(particles(rf), by=weight, rev=true)
sort!(particles(rf_crp), by=weight)
```

# TODO: examples of high/low prior/likelihood

```julia
# high likelihood, high prior:
@df recalled1 arena(:x, :y, group=assignments(first(particles(rf))), markerstrokecolor=GrayA(1., 0.5))
```

```julia
# same prior, low likelihood:
@df recalled1 arena(:x, :y, group=StatsBase.sample([1,2], 200; replace=true))
```

```julia
@df recalled1 arena(:x, :y, group=1:200)
```

# _approach_: bounded rationality

<h2 class="old"><em>Computational-level</em></h2>

## _Algorithmic-level_: Sequential Monte Carlo

* **online** (not batch)
* **finite** uncertainty
* particle filter:
    * Each particle is one hypothetical clustering $z_{1\ldots t}$
    * Update particles in parallel following new data point
    * Re-sample when particles become too homogenous

# Does it _work_?

## Learning clusters

## Recall

## Prediction

# _Learning_ clusters

```julia
@df recalled1 plot(arena(:x, :y, group=assignments(first(particles(rf))), title="Inferred clusters"),
                   arena(:x, :y, group=:block, title="True blocks"),
                   layout=(1,2), size=(800, 400))
```

# _Learning_ clusters

```julia
plot(plot(show_assignment_similarity(rf), title="Inferred clusters"),
     plot(Gray.(@with(recall1, :block .== :block')), title="True blocks"),
     axis=false, aspect_ratio=:equal, layout=(1,2), size=(800,400))
```

# _Recall_

## _Task_

* Immediate recall with mask

## _Model_

* Bayesian cue combination (after e.g., Huttenlocher)
* two cues: thing you saw, and inferred context
* weighted average (by inverse-variance)

```julia
# result:
#   .experiment:
#     .data:
#     - (recall1, recall2, ...)
#     - (pred1, pred2, ...)
#   .result
#   - (recalled1, predicted1)
#   - (recalled2, predicted2)
#   - ...

function add_params!(df::AbstractDataFrame, params)
    for (k,v) in params
        df[k] = v
    end
    df
end

# extract input data (which is by subject) and predictions
recalled(r) =
    add_params!(hcat(vcat(r.experiment.data[1]...),
                     vcat(first.(r.result)...)),
                r.experiment.params)

predicted(r) =
    add_params!(hcat(vcat(r.experiment.data[2]...),
                     vcat(last.(r.result)...)[[:xys_mod]]),
                r.experiment.params)
```

```julia
recalled_all = mapreduce(recalled, vcat, results)
```

```julia
import .Experiments: cosinesim
cosinesim(d) = cosinesim(d,d)

recalled_summaries = by(recalled_all, [:α, :ρ, :Sσ], d -> DataFrame(cos=cosinesim(d), mse=Experiments.mse(d,d)))
```

```julia
# get a good parameter setting (one where cosine similarity is high and Sσ is high enough to see)
@show rec_params_good = @_ recalled_summaries |>
    @where(_, :Sσ .≈ 0.01) |>
    sort!(_, :cos, rev=true) |> 
    first(_, 1) 

# average over iterations
recalled_good = @_ rec_params_good |>
    join(recalled_all, _, on=[:α, :ρ, :Sσ]) |>
    @by(_, [:subjid1, :block, :rep, :rep_number, :respnr, :rad, :var, :x, :y, :x_resp, :y_resp],
        x_mod = mean(:x_mod), y_mod = mean(:y_mod)) 


recalled1 = @_ recalled_good |>
    @where(_, :subjid1 .== 7)
```

```julia
@df recalled1 arena(:x, :y, quiver=(:x_resp .- :x, :y_resp .- :y), seriestype=:quiver)
@df recalled1 quiver!(:x, :y, quiver=(:x_mod .- :x, :y_mod .- :y), line_z=:rep, color=:grays)
```

```julia
baseline!(df) = @_ df |>
    @transform(_, gt_rho_avg = rho.(:x,:y) .> mean(rho.(:x, :y))) |>
    # @with(_, (hcat(:x,:y) .* ifelse.(:gt_rho_avg, -1, 1))')
    @transform(_, cos_center = 1 .- Distances.colwise(CosineDist(),
                                                      -hcat(:x, :y)', 
                                                      hcat(:x_resp.-:x, :y_resp.-:y)'),
                  cos_mean_rho = 1 .- Distances.colwise(CosineDist(),
                                                        (hcat(:x,:y) .* ifelse.(:gt_rho_avg, -1, 1))',
                                                        hcat(:x_resp.-:x, :y_resp.-:y)'),
                  cos_mod = 1 .- Distances.colwise(CosineDist(),
                                                   hcat(:x_mod.-:x, :y_mod.-:y)',
                                                   hcat(:x_resp.-:x, :y_resp.-:y)'))


baseline_good = baseline!(recalled_good)
bs_center, bs_avgrho = @_ baseline_good |>
    @where(_, (!isnan).(:cos_center)) |>    # there is one trial with zero deviation...
    @with(_, (mean(:cos_center), mean(:cos_mean_rho)))
```

* works best for high stickiness, low clustering (fewer clusters).  but all work about the same

# _Recall_: <span class="model">model</span>

```julia
size2 = (900,400)

p1 = @df recalled1 arena(:x, :y, quiver=(:x_mod.-:x, :y_mod.-:y), seriestype=:quiver, label="Model",
                         layout=@layout([a{0.5w} _]), size=size2)
```

# _Recall_: <span class="model">model</span> + behavior

```julia
@df recalled1 quiver!(:x, :y, quiver=(:x_resp.-:x, :y_mod.-:y), color=GrayA(0.0, 0.3), label="Behavior", subplot=1)
```

# _Recall_: <span class="model">model</span> + behavior

```julia
@_ baseline_good |>
    @where(_, :subjid1 .== 7) |>
    @df _ begin
        plot(arena(:x, :y, quiver=(:x_mod.-:x, :y_mod.-:y), seriestype=:quiver),
             histogram(:cos_mod, bins=20, normalized=true, yaxis=false, label="", 
                       legend=false,
                       aspect_ratio=:equal, 
                       alpha=0.5),
             layout=@layout([a{0.5w} b]), size=size2)
        quiver!(:x, :y, quiver=(:x_resp.-:x, :y_mod.-:y), color=GrayA(0.0, 0.3), label="Behavior", subplot=1)
        vline!([cosinesim(baseline_good)], subplot=2)
        vline!([0], subplot=2, color=:black)
        annotate!(cosinesim(baseline_good)*1.2, 1, text("Mean=$(round(cosinesim(baseline_good), digits=2))", 10, :left),
                  subplot=2)
        title!("Cosine similarity", subplot=2)
    end
```

# _Recall_: overall fit

depends on _stickiness_ (<span class="blue">low</span> &lt; <span class="red">medium</span> &lt; <span class="green">high</span>) and eagerness to create _new clusters_


```julia
@df(@where(recalled_summaries, :Sσ .≈ 0.01),
    plot(:α, :cos, group=:ρ, xscale=:log10, 
         xlims=(10^-2.5, 10^1.5), ylims=(0, 0.115), seriestype=:line,
         xlabel="Eagerness to create new clusters (DP concentration)",
         ylabel=("Cosine sim. with behavior"),
        legend=:bottomright, legend_title="Stickiness", line=2))
```

# _Recall_: baselines

consistently **&approx; known clusters** and **&gt; center/mean radius**

```julia
# @df(@where(recalled_summaries, :Sσ .≈ 0.01),
#     plot(:α, :cos, group=:ρ, xscale=:log10, 
#          xlims=(10^-2.5, 10^1.5), ylims=(0, 0.115), seriestype=:line,
#          xlabel="Eagerness to create new clusters (DP concentration)",
#          ylabel=("Cosine sim. with behavior"),
#         legend=:bottomright, legend_title="Stickiness", line=2))

baseline_x = [10^-2.1, 10^1.1]

function plot_baseline!(y, label)
    plot!(baseline_x, ones(2)*y, color=Gray(0.7), label="")
    annotate!(baseline_x[end]*1.05, y, label)
end

plot_baseline!(cosinesim(known_recalled), text("Known\nclusters", 10, RGB(Gray(0.7)), :left, :bottom))

plot_baseline!(bs_center, text("Center", 10, RGB(Gray(0.7)), :left, :bottom))
plot_baseline!(bs_avgrho, text("Mean rad.", 10, RGB(Gray(0.7)), :left, :top))
```

```julia
@_ baseline_good |>
    @where(_, (!isnan).(:cos_center)) |>
    @by(_, :rad, 
        mod_gt_center = mean(:cos_mod .> :cos_center), 
        mod_minus_center = mean(:cos_mod .- :cos_center),
        mod_gt_meanrho = mean(:cos_mod .> :cos_mean_rho),
        mod_minus_meanrho = mean(:cos_mod .- :cos_mean_rho)) |>
    sort!(_, :rad)
```

```julia
@_ baseline_good |>
    @where(_, (!isnan).(:cos_center)) |>
    @by(_, :subjid1, 
        mod_gt_center = mean(:cos_mod .> :cos_center), 
        mod_minus_center = mean(:cos_mod .- :cos_center),
        mod_gt_meanrho = mean(:cos_mod .> :cos_mean_rho),
        mod_minus_meanrho = mean(:cos_mod .- :cos_mean_rho))
```

```julia
@_ baseline_good |>
    @where(_, (!isnan).(:cos_center), :block .> 1) |>
    @by(_, :rep, 
        mod_gt_center = mean(:cos_mod .> :cos_center), 
        mod_minus_center = mean(:cos_mod .- :cos_center),
        mod_gt_meanrho = mean(:cos_mod .> :cos_mean_rho),
        mod_minus_meanrho = mean(:cos_mod .- :cos_mean_rho)) |>
    @df(_, scatter(:rep, hcat(:mod_minus_center, :mod_minus_meanrho), smooth=true,
                   label=["center", "avg. radius"], legend_title="Model bias vs. bias towards", legend=:topleft,
                   xlabel="Trials into cluster"))
```

```julia
@_ baseline_good |>
    @where(_, (!isnan).(:cos_center), :block .> 1) |>
    @by(_, :rep, 
        mod_gt_center = mean(:cos_mod .> :cos_center), 
        mod_minus_center = mean(:cos_mod .- :cos_center),
        mod_gt_meanrho = mean(:cos_mod .> :cos_mean_rho),
        mod_minus_meanrho = mean(:cos_mod .- :cos_mean_rho)) |>
    sort!(_, :rep)
```

```julia
@_ baseline_good |>
    @where(_, (!isnan).(:cos_center)) |>
    @where(_, :block .> 1) |>
    @by(_, [:rep], 
        mod_gt_center = mean(:cos_mod .> :cos_center), 
        mod_minus_center = mean(:cos_mod .- :cos_center),
        mod_gt_meanrho = mean(:cos_mod .> :cos_mean_rho),
        mod_minus_meanrho = mean(:cos_mod .- :cos_mean_rho)) |>
    @df(_, plot(scatter(:rep, :mod_gt_center, smooth=true),
                scatter(:rep, :mod_gt_meanrho, smooth=true),
                size=(800, 400), legend=false, link=:y))
```

# _Prediction_

## _Task_

* Interleaved with recall task (every three trials)
* Predict location 1, 5, or 10 trials in future

## _Model_

* Sample one particle
* Sample $n$ steps from particle's sticky Chinese Restaurant Process prior on states
* Draw one point from posterior predictive of sampled cluster

```julia
predicted_all = @_ mapreduce(predicted, vcat, results) |>
    @by(_, [:subjid1, :block, :respnr, :pred, :theta_resp, :rho_resp, :α, :ρ], xys_mod = Ref(reduce(vcat, :xys_mod))) |>
    @transform(_, subjid1 = Int.(:subjid1),
                  block = Int.(:block), 
                  respnr = Int.(:respnr),
                  x_resp = cos.(:theta_resp) .* :rho_resp./2π,
                  y_resp = sin.(:theta_resp) .* :rho_resp./2π,
                  xys_mod = map(getindex, :xys_mod))

predicted_good = @_ predicted_all |>
    join(_, deletecols!(copy(rec_params_good), :Sσ), on=[:α, :ρ]) |>
    join(_, @select(recall, :subjid1, :block, :respnr, :x, :y), 
            on=[:subjid1, :block, :respnr]) |>
    sort!(_, (:subjid1, :respnr, :pred))

predicted1 = @where(predicted_good, :subjid1 .== 7);
```

```julia
white_to_black = cgrad([Gray(.9), Gray(0)])

function plot_prediction_task(xys_mod::Matrix, 
                              x::Float64, y::Float64, 
                              x_resp::Float64, y_resp::Float64)
    p = arena(xys_mod[:,1], xys_mod[:,2], seriestype=:histogram2d, color=white_to_black, 
              bins=range(-1.2, stop=1.2, length=50), lims=(-1.2, 1.2))
    scatter!(p, [x], [y], color=:white, markerstrokecolor=Gray(.5))
    scatter!(p, [x_resp], [y_resp], color=:red)
    return p
end

function plot_prediction_task(d::AbstractDataFrame, args...; title=true, kwargs...)
    plots = let plots = [], title=title
        @byrow! d begin
            push!(plots, plot_prediction_task(:xys_mod, :x, :y, :x_resp, :y_resp))
            title!(plots[end], title ? "Trial $(:respnr) (+$(:pred))" : "")
        end
        plots
    end
    plot(plots..., args...; kwargs...)
end
```

# _Prediction_: model + <span style="color:red">behavior</span>

```julia
let anim = Animation()
    @byrow! predicted1 begin
        plot_prediction_task(:xys_mod, :x, :y, :x_resp, :y_resp)
        title!("Trial $(:respnr) (+$(:pred))")
        frame(anim)
    end
    gif(anim, "figures/pred1.gif", fps=1)
end
```

```julia
let anim = Animation(), rx = recalled1[:x], ry = recalled1[:y], rnr = recalled1[:respnr]
    @byrow! predicted1 begin
        p1 = plot_prediction_task(:xys_mod, :x, :y, :x_resp, :y_resp)
        title!("Trial $(:respnr) (+$(:pred))")
        p2 = arena(rx[rnr.<=:respnr], ry[rnr.<=:respnr], lims=(-1.2, 1.2), 
                   markeralpha=0.25, color=Gray(0), title="Studied")
        plot(p1, p2, size=(800, 400))
        frame(anim)
    end
    gif(anim, "figures/pred2.gif", fps=1)
end
```

* How far away from the last point do people typically guess?

# _Prediction_: model + <span style="color:red">behavior</span>

```julia
@_ predicted1 |>
    @where(_, :respnr .== 23) |>
    plot_prediction_task(_, title=true, size=(800,400))
```

# _Prediction_: model + <span style="color:red">behavior</span>

```julia
plot_prediction_task(predicted1[end-1:end,:], size=(800, 400))
```

# _Prediction_: average distance

**model &approx; behavior**: distance of predictions at <span class="blue">1 trial</span> &lt; <span class="red">5 trials</span> &lt; <span class="green">10 trials</span> 

```julia
prediction_deviations = @_ predicted_good |>
    @transform(_, resp_dev = rho.(:x.-:x_resp, :y.-:y_resp), 
                  mod_dev = mean.(pairwise.(Ref(Euclidean()), 
                                            transpose.(hcat.(:x, :y)),
                                            transpose.(:xys_mod))))

@_ prediction_deviations |>
    @by(_, [:pred], resp_dev = mean(:resp_dev), mod_dev = mean(:mod_dev)) |>
    @df(_, begin 
        scatter(:resp_dev, :mod_dev, markersize=8, xlabel="Response deviation (from last recall)", ylabel="Model predicted", 
                group=:pred, legend=:bottomright, aspect_ratio=:equal, 
                legend_title="Prediction for next")
        #scatter!(:resp_dev, :mod_dev, markeralpha=0, color=:black, label="", smooth=true)
        end)


plot!(x->x, color=GrayA(0.2, 0.2), label="")
```

* even _within_ a prediction offset, there's some variability in the deviation of the prediction from the current point.
* ...and there's a postive relation between what the model 

# _Prediction_: single trial distance

captures variation _within_ prediction horizons

```julia
@_ prediction_deviations |>
    @by(_, [:subjid1, :block, :respnr, :pred], resp_dev = mean(:resp_dev), mod_dev = mean(:mod_dev)) |>
    @df(_, scatter(:resp_dev, :mod_dev, xlabel="Response deviation", ylabel="Model predicted", 
                   legend_title = "Predictions for next", legend=:bottomright,
                   group=:pred, smooth=true, markerstrokecolor=:white, markeralpha=.3, line=2, aspect_ratio = :equal,
                   size=(800,400)))

plot!(range(0,stop=1,length=100), x->x, color=GrayA(0.2, 0.2), label="")

# @_ prediction_deviations |>
#     @by(_, [:pred], resp_dev = mean(:resp_dev), mod_dev = mean(:mod_dev)) |>
#     @df(_, scatter!(:resp_dev, :mod_dev))
```

# what have _we_ learned

* people _pick up_ and _use_ structure in recall and prediction
* sequential Bayesian model _learns_ clusters online
* learned clusters captures behavior:
    * _recall_: better than simple baselines
    * _prediction_: distance from last last studied location

# what's _left_

* budget for uncertainty (number of particles)
* online learning of stickiness/clustering/prior on cluster mean/variance
* apply to new data (clearer clusters and no clusters)
